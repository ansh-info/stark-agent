# STaRK Benchmark Evaluation

A hierarchical agent system for evaluating LLM retrieval performance on semi-structured knowledge bases.

## ğŸ“‹ Overview

STaRK (Semi-structured Text and Relational Knowledge) is a comprehensive benchmark designed to evaluate how well large language models (LLMs) and retrieval systems work with semi-structured knowledge bases (SKBs). This project implements a hierarchical LLM-powered agent system to process, evaluate, and visualize the performance of retrieval systems on the STaRK benchmark.

## ğŸŒŸ Features

- **Hierarchical Agent System**: Multi-agent architecture with specialized agents for different tasks
- **Comprehensive Evaluation**: Calculate standard retrieval metrics (MRR, MAP, NDCG, Recall@K, etc.)
- **Interactive Knowledge Graph**: Visualize node relationships and similarities
- **Streamlit UI**: User-friendly interface for uploading data, running evaluations, and analyzing results
- **Memory Optimization**: Efficiently process large embedding datasets
- **LLM-powered Analysis**: Natural language interface to query and understand results

## ğŸ§  Agent Architecture

The system implements a hierarchical agent structure:

- **Main Agent**: Supervisory agent that routes user queries and orchestrates tasks
- **StarkQA Agent**: Specialized agent focused on evaluation tasks
- **Tools**: Specialized functions for evaluation, metrics calculation, and visualization

## ğŸ“Š Evaluation Pipeline

1. **Data Loading**: Process query and node embeddings from parquet files
2. **Similarity Computation**: Calculate similarities between queries and nodes
3. **Metrics Calculation**: Compute standard retrieval metrics (MRR, MAP, Recall@K)
4. **Knowledge Graph Generation**: Create interactive visualizations of node relationships
5. **Result Analysis**: Provide natural language insights about evaluation results

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- LangChain/LangGraph
- OpenAI API key (for GPT-4-mini)
- PyTorch
- Streamlit

### Installation

```bash
# Clone the repository
git clone https://github.com/ansh-info/stark-agent.git
cd stark-agent

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up your OpenAI API key
export OPENAI_API_KEY=your_api_key_here  # On Windows: set OPENAI_API_KEY=your_api_key_here
```

### Running the Application

```bash
# Start the Streamlit app
streamlit run app/stark_app.py
```

## ğŸ“ Usage

1. **Upload Files**:

   - Query Embeddings (parquet format)
   - Node Embeddings (parquet format)

2. **Configure Settings**:

   - Batch Size: Number of queries to process at once
   - Processing Chunk Size: Size of data chunks for memory optimization
   - Evaluation Split: Data split to evaluate on

3. **Run Evaluation**:

   - Click "Run Evaluation" to start the process
   - Monitor progress in the UI

4. **Analyze Results**:
   - View metrics visualization
   - Explore the knowledge graph
   - Ask questions in natural language

## ğŸ“ˆ Evaluation Metrics

The system calculates and displays the following metrics:

- **MRR (Mean Reciprocal Rank)**: Measures where first correct answers appear in ranking
- **MAP (Mean Average Precision)**: Measures precision across all relevant items
- **R-Precision**: Precision at the position equal to number of relevant items
- **Recall@K**: Proportion of relevant items found in top K results
- **Hit@K**: Whether any relevant item appears in top K results

## ğŸ” Knowledge Graph Visualization

The interactive knowledge graph shows:

- **Nodes**: Representing entities in the knowledge base
- **Edges**: Showing strong similarity connections
- **Colors**: Differentiating node types
- **Tooltips**: Showing detailed node information

## ğŸ§© Project Structure

```
.
â”œâ”€â”€ .env                  # Environment variables
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main_agent.py     # Main supervisory agent
â”‚   â””â”€â”€ stark_agent.py    # StarkQA evaluation agent
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.py         # Configuration settings
â”œâ”€â”€ state/
â”‚   â””â”€â”€ shared_state.py   # Shared state management
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_stark_evaluation.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ stark/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ evaluation_retrival.py  # Core evaluation tool
â”œâ”€â”€ app/
â”‚   â””â”€â”€ stark_app.py             # Streamlit app
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ llm.py            # LLM utilities
```

## ğŸ¤ Contributing

We welcome contributions! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgements

- STaRK benchmark creators (Stanford SNAP Group)
- LangChain and LangGraph for agent framework
- Streamlit for the UI framework
- PyTorch and TorchMetrics for evaluation metrics
